{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5922c56f",
   "metadata": {},
   "source": [
    "# Word2Vec on Lending Club dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2c91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f460afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "import fasttext\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e9cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87969209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6238cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e8fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378df2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db62d0",
   "metadata": {},
   "source": [
    "### Retrieving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513ea6",
   "metadata": {},
   "source": [
    "#### DF 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3d2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df7 = pd.read_csv('df7.csv')\n",
    "df7 = pd.read_csv('df7.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ed7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>target</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36months</td>\n",
       "      <td>0.1065</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>10</td>\n",
       "      <td>RENT</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Computer</td>\n",
       "      <td>860</td>\n",
       "      <td>AZ</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1985</td>\n",
       "      <td>735.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13648.0</td>\n",
       "      <td>0.837</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5863.155187</td>\n",
       "      <td>5833.84</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>863.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2015</td>\n",
       "      <td>171.62</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>744.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>__label__1 1077501 1296599.0 5000.0 5000.0 497...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60months</td>\n",
       "      <td>0.1527</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>car</td>\n",
       "      <td>bike</td>\n",
       "      <td>309</td>\n",
       "      <td>GA</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1999</td>\n",
       "      <td>740.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>0.094</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008.710000</td>\n",
       "      <td>1008.71</td>\n",
       "      <td>456.46</td>\n",
       "      <td>435.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.08</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2013</td>\n",
       "      <td>119.66</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>499.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>__label__2 1077430 1314167.0 2500.0 2500.0 250...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv      term  \\\n",
       "0  1077501  1296599.0     5000.0       5000.0           4975.0  36months   \n",
       "1  1077430  1314167.0     2500.0       2500.0           2500.0  60months   \n",
       "\n",
       "   int_rate  installment grade sub_grade emp_title  emp_length home_ownership  \\\n",
       "0    0.1065       162.87     B        B2   Unknown          10           RENT   \n",
       "1    0.1527        59.83     C        C4     Ryder           1           RENT   \n",
       "\n",
       "   annual_inc verification_status  issue_d  pymnt_plan      purpose     title  \\\n",
       "0     24000.0            Verified     2011       False  credit_card  Computer   \n",
       "1     30000.0     Source Verified     2011       False          car      bike   \n",
       "\n",
       "   zip_code addr_state    dti  delinq_2yrs  earliest_cr_line  fico_range_low  \\\n",
       "0       860         AZ  27.65          0.0              1985           735.0   \n",
       "1       309         GA   1.00          0.0              1999           740.0   \n",
       "\n",
       "   fico_range_high  inq_last_6mths  mths_since_last_delinq  \\\n",
       "0            739.0             1.0                   500.0   \n",
       "1            744.0             5.0                   500.0   \n",
       "\n",
       "   mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  \\\n",
       "0                   222.0       3.0      0.0    13648.0       0.837   \n",
       "1                   222.0       3.0      0.0     1687.0       0.094   \n",
       "\n",
       "   total_acc  out_prncp  out_prncp_inv  total_pymnt  total_pymnt_inv  \\\n",
       "0        9.0        0.0            0.0  5863.155187          5833.84   \n",
       "1        4.0        0.0            0.0  1008.710000          1008.71   \n",
       "\n",
       "   total_rec_prncp  total_rec_int  total_rec_late_fee  recoveries  \\\n",
       "0          5000.00         863.16                 0.0        0.00   \n",
       "1           456.46         435.17                 0.0      117.08   \n",
       "\n",
       "   collection_recovery_fee  last_pymnt_d  last_pymnt_amnt  next_pymnt_d  \\\n",
       "0                     0.00          2015           171.62             0   \n",
       "1                     1.11          2013           119.66             0   \n",
       "\n",
       "   last_credit_pull_d  last_fico_range_high  last_fico_range_low  \\\n",
       "0                2016                 744.0                740.0   \n",
       "1                2016                 499.0                  0.0   \n",
       "\n",
       "   collections_12_mths_ex_med  acc_now_delinq  chargeoff_within_12_mths  \\\n",
       "0                       False           False                     False   \n",
       "1                       False           False                     False   \n",
       "\n",
       "   delinq_amnt  pub_rec_bankruptcies  tax_liens      target  \\\n",
       "0          0.0                   0.0      False  __label__1   \n",
       "1          0.0                   0.0      False  __label__2   \n",
       "\n",
       "                                             content  \n",
       "0  __label__1 1077501 1296599.0 5000.0 5000.0 497...  \n",
       "1  __label__2 1077430 1314167.0 2500.0 2500.0 250...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9963f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0a55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation metrics for each fold\n",
    "dataset_used = []\n",
    "model_used = []\n",
    "data_balancing_technique = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903b5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_metrics = pd.DataFrame()\n",
    "\n",
    "combined_metrics = pd.DataFrame(columns=['dataset', 'model', 'data balancing technique', 'fold', 'precision_1','precision_2','recall_1','recall_2','f1-score_1','f1-score_2','support_1','support_2','TP','FP','TN','FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cda7c",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf6e11",
   "metadata": {},
   "source": [
    "## DF7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f99b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.92\n",
      "Mean Precision: 0.92\n",
      "Mean Recall: 0.92\n",
      "Mean F1-Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df7['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df7['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())\n",
    "    precision = precision_score(y_test, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred.round())\n",
    "    \n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF7',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'None',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5cd58d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.922721</td>\n",
       "      <td>0.863510</td>\n",
       "      <td>0.986550</td>\n",
       "      <td>0.507365</td>\n",
       "      <td>0.953569</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>3594</td>\n",
       "      <td>301</td>\n",
       "      <td>310</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.921282</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.989262</td>\n",
       "      <td>0.506431</td>\n",
       "      <td>0.954063</td>\n",
       "      <td>0.645492</td>\n",
       "      <td>3632.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>3593</td>\n",
       "      <td>307</td>\n",
       "      <td>315</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.920741</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.954133</td>\n",
       "      <td>0.658730</td>\n",
       "      <td>3614.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3578</td>\n",
       "      <td>308</td>\n",
       "      <td>332</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.877863</td>\n",
       "      <td>0.986704</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.953554</td>\n",
       "      <td>0.665381</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>3562</td>\n",
       "      <td>299</td>\n",
       "      <td>345</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.988929</td>\n",
       "      <td>0.542902</td>\n",
       "      <td>0.955475</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>3573</td>\n",
       "      <td>293</td>\n",
       "      <td>348</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0.927755</td>\n",
       "      <td>0.893827</td>\n",
       "      <td>0.988099</td>\n",
       "      <td>0.565625</td>\n",
       "      <td>0.956976</td>\n",
       "      <td>0.692823</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3570</td>\n",
       "      <td>278</td>\n",
       "      <td>362</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.913378</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.991929</td>\n",
       "      <td>0.487879</td>\n",
       "      <td>0.951034</td>\n",
       "      <td>0.636993</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3564</td>\n",
       "      <td>338</td>\n",
       "      <td>322</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.917656</td>\n",
       "      <td>0.926121</td>\n",
       "      <td>0.992185</td>\n",
       "      <td>0.523881</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.669209</td>\n",
       "      <td>3583.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>3555</td>\n",
       "      <td>319</td>\n",
       "      <td>351</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>0.912487</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.993038</td>\n",
       "      <td>0.483384</td>\n",
       "      <td>0.951060</td>\n",
       "      <td>0.635551</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>3566</td>\n",
       "      <td>342</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.917478</td>\n",
       "      <td>0.911681</td>\n",
       "      <td>0.991415</td>\n",
       "      <td>0.498442</td>\n",
       "      <td>0.953015</td>\n",
       "      <td>0.644512</td>\n",
       "      <td>3611.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>3580</td>\n",
       "      <td>322</td>\n",
       "      <td>320</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset                         model data balancing technique  fold  \\\n",
       "0     DF7  Word2Vec - Linear Regression                     None     1   \n",
       "1     DF7  Word2Vec - Linear Regression                     None     2   \n",
       "2     DF7  Word2Vec - Linear Regression                     None     3   \n",
       "3     DF7  Word2Vec - Linear Regression                     None     4   \n",
       "4     DF7  Word2Vec - Linear Regression                     None     5   \n",
       "5     DF7  Word2Vec - Linear Regression                     None     6   \n",
       "6     DF7  Word2Vec - Linear Regression                     None     7   \n",
       "7     DF7  Word2Vec - Linear Regression                     None     8   \n",
       "8     DF7  Word2Vec - Linear Regression                     None     9   \n",
       "9     DF7  Word2Vec - Linear Regression                     None    10   \n",
       "\n",
       "   precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "0     0.922721     0.863510  0.986550  0.507365    0.953569    0.639175   \n",
       "1     0.921282     0.889831  0.989262  0.506431    0.954063    0.645492   \n",
       "2     0.920741     0.902174  0.990039  0.518750    0.954133    0.658730   \n",
       "3     0.922559     0.877863  0.986704  0.535714    0.953554    0.665381   \n",
       "4     0.924211     0.896907  0.988929  0.542902    0.955475    0.676385   \n",
       "5     0.927755     0.893827  0.988099  0.565625    0.956976    0.692823   \n",
       "6     0.913378     0.917379  0.991929  0.487879    0.951034    0.636993   \n",
       "7     0.917656     0.926121  0.992185  0.523881    0.953467    0.669209   \n",
       "8     0.912487     0.927536  0.993038  0.483384    0.951060    0.635551   \n",
       "9     0.917478     0.911681  0.991415  0.498442    0.953015    0.644512   \n",
       "\n",
       "   support_1  support_2    TP   FP   TN  FN  \n",
       "0     3643.0      611.0  3594  301  310  49  \n",
       "1     3632.0      622.0  3593  307  315  39  \n",
       "2     3614.0      640.0  3578  308  332  36  \n",
       "3     3610.0      644.0  3562  299  345  48  \n",
       "4     3613.0      641.0  3573  293  348  40  \n",
       "5     3613.0      640.0  3570  278  362  43  \n",
       "6     3593.0      660.0  3564  338  322  29  \n",
       "7     3583.0      670.0  3555  319  351  28  \n",
       "8     3591.0      662.0  3566  342  320  25  \n",
       "9     3611.0      642.0  3580  322  320  31  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'None') & (combined_metrics['dataset'] == 'DF7')]#.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bdf77",
   "metadata": {},
   "source": [
    "### Word2Vec Linear Regression with df7 data and imbalance data tackling (RandomUnderSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b575ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.89\n",
      "Mean Precision: 0.89\n",
      "Mean Recall: 0.89\n",
      "Mean F1-Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df7['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df7['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    # Apply random undersampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Apply random undersampling to test data\n",
    "    X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_resampled)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test_resampled, y_pred.round())\n",
    "    precision = precision_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test_resampled, y_pred.round())\n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test_resampled, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test_resampled, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF7',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'Random Under Sampler',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f08b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86250</td>\n",
       "      <td>0.898625</td>\n",
       "      <td>0.903437</td>\n",
       "      <td>0.855974</td>\n",
       "      <td>0.882494</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>552</td>\n",
       "      <td>88</td>\n",
       "      <td>523</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.85303</td>\n",
       "      <td>0.898973</td>\n",
       "      <td>0.905145</td>\n",
       "      <td>0.844051</td>\n",
       "      <td>0.878315</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>563</td>\n",
       "      <td>97</td>\n",
       "      <td>525</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "10     DF7  Word2Vec - Linear Regression     Random Under Sampler     1   \n",
       "11     DF7  Word2Vec - Linear Regression     Random Under Sampler     2   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "10      0.86250     0.898625  0.903437  0.855974    0.882494    0.876781   \n",
       "11      0.85303     0.898973  0.905145  0.844051    0.878315    0.870647   \n",
       "\n",
       "    support_1  support_2   TP  FP   TN  FN  \n",
       "10      611.0      611.0  552  88  523  59  \n",
       "11      622.0      622.0  563  97  525  59  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'Random Under Sampler') & (combined_metrics['dataset'] == 'DF7')].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10646",
   "metadata": {},
   "source": [
    "### Word2Vec Linear Regression with df7 data and imbalance data tackling (RandomOverSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c500a1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.89\n",
      "Mean Precision: 0.89\n",
      "Mean Recall: 0.89\n",
      "Mean F1-Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df7['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df7['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    # Apply random undersampling\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Apply random undersampling to test data\n",
    "    X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_resampled)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test_resampled, y_pred.round())\n",
    "    precision = precision_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test_resampled, y_pred.round())\n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test_resampled, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test_resampled, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF7',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'Random Over Sampler',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca7e58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869427</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.865794</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.878008</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>546</td>\n",
       "      <td>82</td>\n",
       "      <td>529</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.854354</td>\n",
       "      <td>0.908304</td>\n",
       "      <td>0.914791</td>\n",
       "      <td>0.844051</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>569</td>\n",
       "      <td>97</td>\n",
       "      <td>525</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.834808</td>\n",
       "      <td>0.877076</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.858877</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>566</td>\n",
       "      <td>112</td>\n",
       "      <td>528</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.848752</td>\n",
       "      <td>0.891269</td>\n",
       "      <td>0.897516</td>\n",
       "      <td>0.840062</td>\n",
       "      <td>0.872453</td>\n",
       "      <td>0.864908</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>578</td>\n",
       "      <td>103</td>\n",
       "      <td>541</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.841040</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.907956</td>\n",
       "      <td>0.828393</td>\n",
       "      <td>0.873218</td>\n",
       "      <td>0.862713</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>582</td>\n",
       "      <td>110</td>\n",
       "      <td>531</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.861237</td>\n",
       "      <td>0.888169</td>\n",
       "      <td>0.892188</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.876439</td>\n",
       "      <td>0.871917</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>571</td>\n",
       "      <td>92</td>\n",
       "      <td>548</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>594</td>\n",
       "      <td>99</td>\n",
       "      <td>561</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.867792</td>\n",
       "      <td>0.915739</td>\n",
       "      <td>0.920896</td>\n",
       "      <td>0.859701</td>\n",
       "      <td>0.893555</td>\n",
       "      <td>0.886836</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>617</td>\n",
       "      <td>94</td>\n",
       "      <td>576</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.850941</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.888218</td>\n",
       "      <td>0.844411</td>\n",
       "      <td>0.869180</td>\n",
       "      <td>0.863320</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>588</td>\n",
       "      <td>103</td>\n",
       "      <td>559</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820334</td>\n",
       "      <td>0.906360</td>\n",
       "      <td>0.917445</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.849338</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>589</td>\n",
       "      <td>129</td>\n",
       "      <td>513</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "20     DF7  Word2Vec - Linear Regression      Random Over Sampler     1   \n",
       "21     DF7  Word2Vec - Linear Regression      Random Over Sampler     2   \n",
       "22     DF7  Word2Vec - Linear Regression      Random Over Sampler     3   \n",
       "23     DF7  Word2Vec - Linear Regression      Random Over Sampler     4   \n",
       "24     DF7  Word2Vec - Linear Regression      Random Over Sampler     5   \n",
       "25     DF7  Word2Vec - Linear Regression      Random Over Sampler     6   \n",
       "26     DF7  Word2Vec - Linear Regression      Random Over Sampler     7   \n",
       "27     DF7  Word2Vec - Linear Regression      Random Over Sampler     8   \n",
       "28     DF7  Word2Vec - Linear Regression      Random Over Sampler     9   \n",
       "29     DF7  Word2Vec - Linear Regression      Random Over Sampler    10   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "20     0.869427     0.890572  0.893617  0.865794    0.881356    0.878008   \n",
       "21     0.854354     0.908304  0.914791  0.844051    0.883540    0.875000   \n",
       "22     0.834808     0.877076  0.884375  0.825000    0.858877    0.850242   \n",
       "23     0.848752     0.891269  0.897516  0.840062    0.872453    0.864908   \n",
       "24     0.841040     0.900000  0.907956  0.828393    0.873218    0.862713   \n",
       "25     0.861237     0.888169  0.892188  0.856250    0.876439    0.871917   \n",
       "26     0.857143     0.894737  0.900000  0.850000    0.878049    0.871795   \n",
       "27     0.867792     0.915739  0.920896  0.859701    0.893555    0.886836   \n",
       "28     0.850941     0.883096  0.888218  0.844411    0.869180    0.863320   \n",
       "29     0.820334     0.906360  0.917445  0.799065    0.866176    0.849338   \n",
       "\n",
       "    support_1  support_2   TP   FP   TN  FN  \n",
       "20      611.0      611.0  546   82  529  65  \n",
       "21      622.0      622.0  569   97  525  53  \n",
       "22      640.0      640.0  566  112  528  74  \n",
       "23      644.0      644.0  578  103  541  66  \n",
       "24      641.0      641.0  582  110  531  59  \n",
       "25      640.0      640.0  571   92  548  69  \n",
       "26      660.0      660.0  594   99  561  66  \n",
       "27      670.0      670.0  617   94  576  53  \n",
       "28      662.0      662.0  588  103  559  74  \n",
       "29      642.0      642.0  589  129  513  53  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'Random Over Sampler') & (combined_metrics['dataset'] == 'DF7')]#.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd948ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4b04f0",
   "metadata": {},
   "source": [
    "## Performance metrics for the Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1158fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.922721</td>\n",
       "      <td>0.863510</td>\n",
       "      <td>0.986550</td>\n",
       "      <td>0.507365</td>\n",
       "      <td>0.953569</td>\n",
       "      <td>0.639175</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>3594</td>\n",
       "      <td>301</td>\n",
       "      <td>310</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.921282</td>\n",
       "      <td>0.889831</td>\n",
       "      <td>0.989262</td>\n",
       "      <td>0.506431</td>\n",
       "      <td>0.954063</td>\n",
       "      <td>0.645492</td>\n",
       "      <td>3632.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>3593</td>\n",
       "      <td>307</td>\n",
       "      <td>315</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.920741</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.990039</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.954133</td>\n",
       "      <td>0.658730</td>\n",
       "      <td>3614.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3578</td>\n",
       "      <td>308</td>\n",
       "      <td>332</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.877863</td>\n",
       "      <td>0.986704</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.953554</td>\n",
       "      <td>0.665381</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>3562</td>\n",
       "      <td>299</td>\n",
       "      <td>345</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.988929</td>\n",
       "      <td>0.542902</td>\n",
       "      <td>0.955475</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>3573</td>\n",
       "      <td>293</td>\n",
       "      <td>348</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0.927755</td>\n",
       "      <td>0.893827</td>\n",
       "      <td>0.988099</td>\n",
       "      <td>0.565625</td>\n",
       "      <td>0.956976</td>\n",
       "      <td>0.692823</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3570</td>\n",
       "      <td>278</td>\n",
       "      <td>362</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.913378</td>\n",
       "      <td>0.917379</td>\n",
       "      <td>0.991929</td>\n",
       "      <td>0.487879</td>\n",
       "      <td>0.951034</td>\n",
       "      <td>0.636993</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3564</td>\n",
       "      <td>338</td>\n",
       "      <td>322</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.917656</td>\n",
       "      <td>0.926121</td>\n",
       "      <td>0.992185</td>\n",
       "      <td>0.523881</td>\n",
       "      <td>0.953467</td>\n",
       "      <td>0.669209</td>\n",
       "      <td>3583.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>3555</td>\n",
       "      <td>319</td>\n",
       "      <td>351</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>0.912487</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.993038</td>\n",
       "      <td>0.483384</td>\n",
       "      <td>0.951060</td>\n",
       "      <td>0.635551</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>3566</td>\n",
       "      <td>342</td>\n",
       "      <td>320</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.917478</td>\n",
       "      <td>0.911681</td>\n",
       "      <td>0.991415</td>\n",
       "      <td>0.498442</td>\n",
       "      <td>0.953015</td>\n",
       "      <td>0.644512</td>\n",
       "      <td>3611.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>3580</td>\n",
       "      <td>322</td>\n",
       "      <td>320</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.898625</td>\n",
       "      <td>0.903437</td>\n",
       "      <td>0.855974</td>\n",
       "      <td>0.882494</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>552</td>\n",
       "      <td>88</td>\n",
       "      <td>523</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.853030</td>\n",
       "      <td>0.898973</td>\n",
       "      <td>0.905145</td>\n",
       "      <td>0.844051</td>\n",
       "      <td>0.878315</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>563</td>\n",
       "      <td>97</td>\n",
       "      <td>525</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.876271</td>\n",
       "      <td>0.885938</td>\n",
       "      <td>0.807813</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.840650</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>567</td>\n",
       "      <td>123</td>\n",
       "      <td>517</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.844118</td>\n",
       "      <td>0.884868</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.835404</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.859425</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>574</td>\n",
       "      <td>106</td>\n",
       "      <td>538</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.892916</td>\n",
       "      <td>0.898596</td>\n",
       "      <td>0.845554</td>\n",
       "      <td>0.875380</td>\n",
       "      <td>0.868590</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>576</td>\n",
       "      <td>99</td>\n",
       "      <td>542</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.853293</td>\n",
       "      <td>0.885621</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.846875</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.865815</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>570</td>\n",
       "      <td>98</td>\n",
       "      <td>542</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.854885</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.901515</td>\n",
       "      <td>0.846970</td>\n",
       "      <td>0.877581</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>595</td>\n",
       "      <td>101</td>\n",
       "      <td>559</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.861972</td>\n",
       "      <td>0.907937</td>\n",
       "      <td>0.913433</td>\n",
       "      <td>0.853731</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>612</td>\n",
       "      <td>98</td>\n",
       "      <td>572</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.858394</td>\n",
       "      <td>0.884194</td>\n",
       "      <td>0.888218</td>\n",
       "      <td>0.853474</td>\n",
       "      <td>0.873051</td>\n",
       "      <td>0.868563</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>588</td>\n",
       "      <td>97</td>\n",
       "      <td>565</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.834746</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.920561</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.875556</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>591</td>\n",
       "      <td>117</td>\n",
       "      <td>525</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869427</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.865794</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.878008</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>546</td>\n",
       "      <td>82</td>\n",
       "      <td>529</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.854354</td>\n",
       "      <td>0.908304</td>\n",
       "      <td>0.914791</td>\n",
       "      <td>0.844051</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>569</td>\n",
       "      <td>97</td>\n",
       "      <td>525</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.834808</td>\n",
       "      <td>0.877076</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.858877</td>\n",
       "      <td>0.850242</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>566</td>\n",
       "      <td>112</td>\n",
       "      <td>528</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.848752</td>\n",
       "      <td>0.891269</td>\n",
       "      <td>0.897516</td>\n",
       "      <td>0.840062</td>\n",
       "      <td>0.872453</td>\n",
       "      <td>0.864908</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>578</td>\n",
       "      <td>103</td>\n",
       "      <td>541</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.841040</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.907956</td>\n",
       "      <td>0.828393</td>\n",
       "      <td>0.873218</td>\n",
       "      <td>0.862713</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>582</td>\n",
       "      <td>110</td>\n",
       "      <td>531</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.861237</td>\n",
       "      <td>0.888169</td>\n",
       "      <td>0.892188</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.876439</td>\n",
       "      <td>0.871917</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>571</td>\n",
       "      <td>92</td>\n",
       "      <td>548</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>594</td>\n",
       "      <td>99</td>\n",
       "      <td>561</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.867792</td>\n",
       "      <td>0.915739</td>\n",
       "      <td>0.920896</td>\n",
       "      <td>0.859701</td>\n",
       "      <td>0.893555</td>\n",
       "      <td>0.886836</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>617</td>\n",
       "      <td>94</td>\n",
       "      <td>576</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.850941</td>\n",
       "      <td>0.883096</td>\n",
       "      <td>0.888218</td>\n",
       "      <td>0.844411</td>\n",
       "      <td>0.869180</td>\n",
       "      <td>0.863320</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>588</td>\n",
       "      <td>103</td>\n",
       "      <td>559</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.820334</td>\n",
       "      <td>0.906360</td>\n",
       "      <td>0.917445</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.849338</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>589</td>\n",
       "      <td>129</td>\n",
       "      <td>513</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "0      DF7  Word2Vec - Linear Regression                     None     1   \n",
       "1      DF7  Word2Vec - Linear Regression                     None     2   \n",
       "2      DF7  Word2Vec - Linear Regression                     None     3   \n",
       "3      DF7  Word2Vec - Linear Regression                     None     4   \n",
       "4      DF7  Word2Vec - Linear Regression                     None     5   \n",
       "5      DF7  Word2Vec - Linear Regression                     None     6   \n",
       "6      DF7  Word2Vec - Linear Regression                     None     7   \n",
       "7      DF7  Word2Vec - Linear Regression                     None     8   \n",
       "8      DF7  Word2Vec - Linear Regression                     None     9   \n",
       "9      DF7  Word2Vec - Linear Regression                     None    10   \n",
       "10     DF7  Word2Vec - Linear Regression     Random Under Sampler     1   \n",
       "11     DF7  Word2Vec - Linear Regression     Random Under Sampler     2   \n",
       "12     DF7  Word2Vec - Linear Regression     Random Under Sampler     3   \n",
       "13     DF7  Word2Vec - Linear Regression     Random Under Sampler     4   \n",
       "14     DF7  Word2Vec - Linear Regression     Random Under Sampler     5   \n",
       "15     DF7  Word2Vec - Linear Regression     Random Under Sampler     6   \n",
       "16     DF7  Word2Vec - Linear Regression     Random Under Sampler     7   \n",
       "17     DF7  Word2Vec - Linear Regression     Random Under Sampler     8   \n",
       "18     DF7  Word2Vec - Linear Regression     Random Under Sampler     9   \n",
       "19     DF7  Word2Vec - Linear Regression     Random Under Sampler    10   \n",
       "20     DF7  Word2Vec - Linear Regression      Random Over Sampler     1   \n",
       "21     DF7  Word2Vec - Linear Regression      Random Over Sampler     2   \n",
       "22     DF7  Word2Vec - Linear Regression      Random Over Sampler     3   \n",
       "23     DF7  Word2Vec - Linear Regression      Random Over Sampler     4   \n",
       "24     DF7  Word2Vec - Linear Regression      Random Over Sampler     5   \n",
       "25     DF7  Word2Vec - Linear Regression      Random Over Sampler     6   \n",
       "26     DF7  Word2Vec - Linear Regression      Random Over Sampler     7   \n",
       "27     DF7  Word2Vec - Linear Regression      Random Over Sampler     8   \n",
       "28     DF7  Word2Vec - Linear Regression      Random Over Sampler     9   \n",
       "29     DF7  Word2Vec - Linear Regression      Random Over Sampler    10   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "0      0.922721     0.863510  0.986550  0.507365    0.953569    0.639175   \n",
       "1      0.921282     0.889831  0.989262  0.506431    0.954063    0.645492   \n",
       "2      0.920741     0.902174  0.990039  0.518750    0.954133    0.658730   \n",
       "3      0.922559     0.877863  0.986704  0.535714    0.953554    0.665381   \n",
       "4      0.924211     0.896907  0.988929  0.542902    0.955475    0.676385   \n",
       "5      0.927755     0.893827  0.988099  0.565625    0.956976    0.692823   \n",
       "6      0.913378     0.917379  0.991929  0.487879    0.951034    0.636993   \n",
       "7      0.917656     0.926121  0.992185  0.523881    0.953467    0.669209   \n",
       "8      0.912487     0.927536  0.993038  0.483384    0.951060    0.635551   \n",
       "9      0.917478     0.911681  0.991415  0.498442    0.953015    0.644512   \n",
       "10     0.862500     0.898625  0.903437  0.855974    0.882494    0.876781   \n",
       "11     0.853030     0.898973  0.905145  0.844051    0.878315    0.870647   \n",
       "12     0.821739     0.876271  0.885938  0.807813    0.852632    0.840650   \n",
       "13     0.844118     0.884868  0.891304  0.835404    0.867069    0.859425   \n",
       "14     0.853333     0.892916  0.898596  0.845554    0.875380    0.868590   \n",
       "15     0.853293     0.885621  0.890625  0.846875    0.871560    0.865815   \n",
       "16     0.854885     0.895833  0.901515  0.846970    0.877581    0.870717   \n",
       "17     0.861972     0.907937  0.913433  0.853731    0.886957    0.880000   \n",
       "18     0.858394     0.884194  0.888218  0.853474    0.873051    0.868563   \n",
       "19     0.834746     0.911458  0.920561  0.817757    0.875556    0.862069   \n",
       "20     0.869427     0.890572  0.893617  0.865794    0.881356    0.878008   \n",
       "21     0.854354     0.908304  0.914791  0.844051    0.883540    0.875000   \n",
       "22     0.834808     0.877076  0.884375  0.825000    0.858877    0.850242   \n",
       "23     0.848752     0.891269  0.897516  0.840062    0.872453    0.864908   \n",
       "24     0.841040     0.900000  0.907956  0.828393    0.873218    0.862713   \n",
       "25     0.861237     0.888169  0.892188  0.856250    0.876439    0.871917   \n",
       "26     0.857143     0.894737  0.900000  0.850000    0.878049    0.871795   \n",
       "27     0.867792     0.915739  0.920896  0.859701    0.893555    0.886836   \n",
       "28     0.850941     0.883096  0.888218  0.844411    0.869180    0.863320   \n",
       "29     0.820334     0.906360  0.917445  0.799065    0.866176    0.849338   \n",
       "\n",
       "    support_1  support_2    TP   FP   TN  FN  \n",
       "0      3643.0      611.0  3594  301  310  49  \n",
       "1      3632.0      622.0  3593  307  315  39  \n",
       "2      3614.0      640.0  3578  308  332  36  \n",
       "3      3610.0      644.0  3562  299  345  48  \n",
       "4      3613.0      641.0  3573  293  348  40  \n",
       "5      3613.0      640.0  3570  278  362  43  \n",
       "6      3593.0      660.0  3564  338  322  29  \n",
       "7      3583.0      670.0  3555  319  351  28  \n",
       "8      3591.0      662.0  3566  342  320  25  \n",
       "9      3611.0      642.0  3580  322  320  31  \n",
       "10      611.0      611.0   552   88  523  59  \n",
       "11      622.0      622.0   563   97  525  59  \n",
       "12      640.0      640.0   567  123  517  73  \n",
       "13      644.0      644.0   574  106  538  70  \n",
       "14      641.0      641.0   576   99  542  65  \n",
       "15      640.0      640.0   570   98  542  70  \n",
       "16      660.0      660.0   595  101  559  65  \n",
       "17      670.0      670.0   612   98  572  58  \n",
       "18      662.0      662.0   588   97  565  74  \n",
       "19      642.0      642.0   591  117  525  51  \n",
       "20      611.0      611.0   546   82  529  65  \n",
       "21      622.0      622.0   569   97  525  53  \n",
       "22      640.0      640.0   566  112  528  74  \n",
       "23      644.0      644.0   578  103  541  66  \n",
       "24      641.0      641.0   582  110  531  59  \n",
       "25      640.0      640.0   571   92  548  69  \n",
       "26      660.0      660.0   594   99  561  66  \n",
       "27      670.0      670.0   617   94  576  53  \n",
       "28      662.0      662.0   588  103  559  74  \n",
       "29      642.0      642.0   589  129  513  53  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8905e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "combined_metrics.to_csv('Output Data/Word2Vec - Linear Regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd98e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
