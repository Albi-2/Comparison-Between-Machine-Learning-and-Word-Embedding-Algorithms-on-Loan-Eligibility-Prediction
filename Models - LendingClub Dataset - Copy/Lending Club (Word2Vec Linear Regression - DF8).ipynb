{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d2c91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f460afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import homogeneity_score, silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import MiniBatchKMeans, DBSCAN\n",
    "import fasttext\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e9cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87969209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6238cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e8fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378df2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db62d0",
   "metadata": {},
   "source": [
    "### Retrieving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10513ea6",
   "metadata": {},
   "source": [
    "#### DF 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3d2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df7 = pd.read_csv('df7.csv')\n",
    "df8 = pd.read_csv('df8.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ed7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>term</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>next_pymnt_d</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>fico_range_high</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_fico_range_high</th>\n",
       "      <th>last_fico_range_low</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>target</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>36months</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>10</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Verified</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Computer</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1985</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>C0Q1</td>\n",
       "      <td>C1Q1</td>\n",
       "      <td>C2Q2</td>\n",
       "      <td>C3Q3</td>\n",
       "      <td>C4Q2</td>\n",
       "      <td>C5Q0</td>\n",
       "      <td>C6Q7</td>\n",
       "      <td>C7Q9</td>\n",
       "      <td>C8Q0</td>\n",
       "      <td>C9Q7</td>\n",
       "      <td>C10Q7</td>\n",
       "      <td>C11Q0</td>\n",
       "      <td>C12Q3</td>\n",
       "      <td>C13Q0</td>\n",
       "      <td>C14Q0</td>\n",
       "      <td>C15Q0</td>\n",
       "      <td>C16Q6</td>\n",
       "      <td>C17Q8</td>\n",
       "      <td>C18Q0</td>\n",
       "      <td>C19Q0</td>\n",
       "      <td>C20Q0</td>\n",
       "      <td>C21Q2</td>\n",
       "      <td>C22Q3</td>\n",
       "      <td>C23Q2</td>\n",
       "      <td>C24Q3</td>\n",
       "      <td>C25Q0</td>\n",
       "      <td>C26Q0</td>\n",
       "      <td>C27Q0</td>\n",
       "      <td>C28Q1</td>\n",
       "      <td>C29Q7</td>\n",
       "      <td>C30Q7</td>\n",
       "      <td>C31Q0</td>\n",
       "      <td>C32Q0</td>\n",
       "      <td>__label__1</td>\n",
       "      <td>__label__1 1077501 36months B B2 Unknown 10  R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>60months</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>2011</td>\n",
       "      <td>False</td>\n",
       "      <td>car</td>\n",
       "      <td>bike</td>\n",
       "      <td>GA</td>\n",
       "      <td>1999</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>C0Q0</td>\n",
       "      <td>C1Q0</td>\n",
       "      <td>C2Q0</td>\n",
       "      <td>C3Q7</td>\n",
       "      <td>C4Q0</td>\n",
       "      <td>C5Q0</td>\n",
       "      <td>C6Q3</td>\n",
       "      <td>C7Q0</td>\n",
       "      <td>C8Q0</td>\n",
       "      <td>C9Q7</td>\n",
       "      <td>C10Q7</td>\n",
       "      <td>C11Q3</td>\n",
       "      <td>C12Q3</td>\n",
       "      <td>C13Q0</td>\n",
       "      <td>C14Q0</td>\n",
       "      <td>C15Q0</td>\n",
       "      <td>C16Q1</td>\n",
       "      <td>C17Q1</td>\n",
       "      <td>C18Q0</td>\n",
       "      <td>C19Q0</td>\n",
       "      <td>C20Q0</td>\n",
       "      <td>C21Q0</td>\n",
       "      <td>C22Q0</td>\n",
       "      <td>C23Q0</td>\n",
       "      <td>C24Q1</td>\n",
       "      <td>C25Q0</td>\n",
       "      <td>C26Q1</td>\n",
       "      <td>C27Q1</td>\n",
       "      <td>C28Q1</td>\n",
       "      <td>C29Q0</td>\n",
       "      <td>C30Q0</td>\n",
       "      <td>C31Q0</td>\n",
       "      <td>C32Q0</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>__label__2 1077430 60months C C4 Ryder  1  REN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      term grade sub_grade emp_title  emp_length home_ownership  \\\n",
       "0  1077501  36months     B        B2   Unknown          10           RENT   \n",
       "1  1077430  60months     C        C4     Ryder           1           RENT   \n",
       "\n",
       "  verification_status  issue_d  pymnt_plan      purpose     title addr_state  \\\n",
       "0            Verified     2011       False  credit_card  Computer         AZ   \n",
       "1     Source Verified     2011       False          car      bike         GA   \n",
       "\n",
       "   earliest_cr_line  last_pymnt_d  next_pymnt_d  last_credit_pull_d  \\\n",
       "0              1985          2015             0                2016   \n",
       "1              1999          2013             0                2016   \n",
       "\n",
       "   collections_12_mths_ex_med  acc_now_delinq  chargeoff_within_12_mths  \\\n",
       "0                       False           False                     False   \n",
       "1                       False           False                     False   \n",
       "\n",
       "   tax_liens loan_amnt funded_amnt funded_amnt_inv int_rate installment  \\\n",
       "0      False      C0Q1        C1Q1            C2Q2     C3Q3        C4Q2   \n",
       "1      False      C0Q0        C1Q0            C2Q0     C3Q7        C4Q0   \n",
       "\n",
       "  annual_inc zip_code   dti delinq_2yrs fico_range_low fico_range_high  \\\n",
       "0       C5Q0     C6Q7  C7Q9        C8Q0           C9Q7           C10Q7   \n",
       "1       C5Q0     C6Q3  C7Q0        C8Q0           C9Q7           C10Q7   \n",
       "\n",
       "  inq_last_6mths mths_since_last_delinq mths_since_last_record open_acc  \\\n",
       "0          C11Q0                  C12Q3                  C13Q0    C14Q0   \n",
       "1          C11Q3                  C12Q3                  C13Q0    C14Q0   \n",
       "\n",
       "  pub_rec revol_bal revol_util total_acc out_prncp out_prncp_inv total_pymnt  \\\n",
       "0   C15Q0     C16Q6      C17Q8     C18Q0     C19Q0         C20Q0       C21Q2   \n",
       "1   C15Q0     C16Q1      C17Q1     C18Q0     C19Q0         C20Q0       C21Q0   \n",
       "\n",
       "  total_pymnt_inv total_rec_prncp total_rec_int total_rec_late_fee recoveries  \\\n",
       "0           C22Q3           C23Q2         C24Q3              C25Q0      C26Q0   \n",
       "1           C22Q0           C23Q0         C24Q1              C25Q0      C26Q1   \n",
       "\n",
       "  collection_recovery_fee last_pymnt_amnt last_fico_range_high  \\\n",
       "0                   C27Q0           C28Q1                C29Q7   \n",
       "1                   C27Q1           C28Q1                C29Q0   \n",
       "\n",
       "  last_fico_range_low delinq_amnt pub_rec_bankruptcies      target  \\\n",
       "0               C30Q7       C31Q0                C32Q0  __label__1   \n",
       "1               C30Q0       C31Q0                C32Q0  __label__2   \n",
       "\n",
       "                                             content  \n",
       "0  __label__1 1077501 36months B B2 Unknown 10  R...  \n",
       "1  __label__2 1077430 60months C C4 Ryder  1  REN...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9963f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0a55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation metrics for each fold\n",
    "dataset_used = []\n",
    "model_used = []\n",
    "data_balancing_technique = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "conf_matrices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903b5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_metrics = pd.DataFrame()\n",
    "\n",
    "combined_metrics = pd.DataFrame(columns=['dataset', 'model', 'data balancing technique', 'fold', 'precision_1','precision_2','recall_1','recall_2','f1-score_1','f1-score_2','support_1','support_2','TP','FP','TN','FN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052cda7c",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf6e11",
   "metadata": {},
   "source": [
    "## DF7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f99b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "Mean Accuracy: 0.98\n",
      "Mean Precision: 0.98\n",
      "Mean Recall: 0.98\n",
      "Mean F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df8['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df8['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred.round())\n",
    "    precision = precision_score(y_test, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred.round())\n",
    "    \n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF7',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'None',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5cd58d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.828151</td>\n",
       "      <td>0.985794</td>\n",
       "      <td>0.905998</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>3643</td>\n",
       "      <td>105</td>\n",
       "      <td>506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.976869</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.988299</td>\n",
       "      <td>0.925734</td>\n",
       "      <td>3632.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>3632</td>\n",
       "      <td>86</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840625</td>\n",
       "      <td>0.986085</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>3614.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3614</td>\n",
       "      <td>102</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.982848</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>3610</td>\n",
       "      <td>126</td>\n",
       "      <td>518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.977279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868955</td>\n",
       "      <td>0.988509</td>\n",
       "      <td>0.929883</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>3613</td>\n",
       "      <td>84</td>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0.976486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864062</td>\n",
       "      <td>0.988103</td>\n",
       "      <td>0.927075</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3613</td>\n",
       "      <td>87</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.972395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.986004</td>\n",
       "      <td>0.916256</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3593</td>\n",
       "      <td>102</td>\n",
       "      <td>558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.969951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.834328</td>\n",
       "      <td>0.984746</td>\n",
       "      <td>0.909683</td>\n",
       "      <td>3583.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>3583</td>\n",
       "      <td>111</td>\n",
       "      <td>559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970270</td>\n",
       "      <td>0.998192</td>\n",
       "      <td>0.999722</td>\n",
       "      <td>0.833837</td>\n",
       "      <td>0.984776</td>\n",
       "      <td>0.908642</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>3590</td>\n",
       "      <td>110</td>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DF7</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819315</td>\n",
       "      <td>0.984192</td>\n",
       "      <td>0.900685</td>\n",
       "      <td>3611.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>3611</td>\n",
       "      <td>116</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset                         model data balancing technique  fold  \\\n",
       "0     DF7  Word2Vec - Linear Regression                     None     1   \n",
       "1     DF7  Word2Vec - Linear Regression                     None     2   \n",
       "2     DF7  Word2Vec - Linear Regression                     None     3   \n",
       "3     DF7  Word2Vec - Linear Regression                     None     4   \n",
       "4     DF7  Word2Vec - Linear Regression                     None     5   \n",
       "5     DF7  Word2Vec - Linear Regression                     None     6   \n",
       "6     DF7  Word2Vec - Linear Regression                     None     7   \n",
       "7     DF7  Word2Vec - Linear Regression                     None     8   \n",
       "8     DF7  Word2Vec - Linear Regression                     None     9   \n",
       "9     DF7  Word2Vec - Linear Regression                     None    10   \n",
       "\n",
       "   precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "0     0.971985     1.000000  1.000000  0.828151    0.985794    0.905998   \n",
       "1     0.976869     1.000000  1.000000  0.861736    0.988299    0.925734   \n",
       "2     0.972551     1.000000  1.000000  0.840625    0.986085    0.913413   \n",
       "3     0.966274     1.000000  1.000000  0.804348    0.982848    0.891566   \n",
       "4     0.977279     1.000000  1.000000  0.868955    0.988509    0.929883   \n",
       "5     0.976486     1.000000  1.000000  0.864062    0.988103    0.927075   \n",
       "6     0.972395     1.000000  1.000000  0.845455    0.986004    0.916256   \n",
       "7     0.969951     1.000000  1.000000  0.834328    0.984746    0.909683   \n",
       "8     0.970270     0.998192  0.999722  0.833837    0.984776    0.908642   \n",
       "9     0.968876     1.000000  1.000000  0.819315    0.984192    0.900685   \n",
       "\n",
       "   support_1  support_2    TP   FP   TN  FN  \n",
       "0     3643.0      611.0  3643  105  506   0  \n",
       "1     3632.0      622.0  3632   86  536   0  \n",
       "2     3614.0      640.0  3614  102  538   0  \n",
       "3     3610.0      644.0  3610  126  518   0  \n",
       "4     3613.0      641.0  3613   84  557   0  \n",
       "5     3613.0      640.0  3613   87  553   0  \n",
       "6     3593.0      660.0  3593  102  558   0  \n",
       "7     3583.0      670.0  3583  111  559   0  \n",
       "8     3591.0      662.0  3590  110  552   1  \n",
       "9     3611.0      642.0  3611  116  526   0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'None') & (combined_metrics['dataset'] == 'DF7')]#.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bdf77",
   "metadata": {},
   "source": [
    "### Word2Vec Linear Regression with df7 data and imbalance data tackling (RandomUnderSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b575ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.97\n",
      "Mean F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df8['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df8['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    # Apply random undersampling\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Apply random undersampling to test data\n",
    "    X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_resampled)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test_resampled, y_pred.round())\n",
    "    precision = precision_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test_resampled, y_pred.round())\n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test_resampled, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test_resampled, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF8',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'Random Under Sampler',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f08b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934985</td>\n",
       "      <td>0.987847</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>0.931260</td>\n",
       "      <td>0.961018</td>\n",
       "      <td>0.958719</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>604</td>\n",
       "      <td>42</td>\n",
       "      <td>569</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.949464</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.946945</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>620</td>\n",
       "      <td>33</td>\n",
       "      <td>589</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "10     DF8  Word2Vec - Linear Regression     Random Under Sampler     1   \n",
       "11     DF8  Word2Vec - Linear Regression     Random Under Sampler     2   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "10     0.934985     0.987847  0.988543  0.931260    0.961018    0.958719   \n",
       "11     0.949464     0.996616  0.996785  0.946945    0.972549    0.971146   \n",
       "\n",
       "    support_1  support_2   TP  FP   TN  FN  \n",
       "10      611.0      611.0  604  42  569   7  \n",
       "11      622.0      622.0  620  33  589   2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'Random Under Sampler') & (combined_metrics['dataset'] == 'DF8')].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10646",
   "metadata": {},
   "source": [
    "### Word2Vec Linear Regression with df7 data and imbalance data tackling (RandomOverSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c500a1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.97\n",
      "Mean F1-Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'df7' is your DataFrame with features and the target column\n",
    "features = df8['content'].apply(lambda x: x.split(' ', 1)[1])  # Drop the target column to get the feature columns\n",
    "target = df8['target'].apply(lambda x: int(x.split(\"__label__\")[1]))  # Target column to predict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([word for word in text if word not in string.punctuation])\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "features = features.apply(preprocess)\n",
    "\n",
    "# Initialize KFold with 10 folds\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "i = 1\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(features):\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "    \n",
    "    #X_train = X_train.apply(preprocess)\n",
    "    #X_test = X_test.apply(preprocess)\n",
    "\n",
    "    sentences = [sentence.split() for sentence in X_train]\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    Word2Vec_model = Word2Vec(sentences, vector_size=100, window=10, min_count=2, workers=4, seed=42)\n",
    "    \n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        words_vecs = [Word2Vec_model.wv[word] for word in words if word in Word2Vec_model.wv]\n",
    "        if len(words_vecs) == 0:\n",
    "            return np.zeros(100)\n",
    "        words_vecs = np.array(words_vecs)\n",
    "        return words_vecs.mean(axis=0)\n",
    "\n",
    "    X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
    "    X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
    "    \n",
    "    # Apply random undersampling\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    #clf = LogisticRegression()\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Apply random undersampling to test data\n",
    "    X_test_resampled, y_test_resampled = rus.fit_resample(X_test, y_test)\n",
    "    \n",
    "    y_pred = clf.predict(X_test_resampled)\n",
    "    \n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    y_pred[y_pred <= 0] = 1\n",
    "    y_pred[y_pred >= 2] = 2\n",
    "    \n",
    "    # Model evaluation\n",
    "    accuracy = accuracy_score(y_test_resampled, y_pred.round())\n",
    "    precision = precision_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    recall = recall_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    f1 = f1_score(y_test_resampled, y_pred.round(), average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test_resampled, y_pred.round())\n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = pd.DataFrame(classification_report(y_test_resampled, y_pred.round(), output_dict=True)).transpose()\n",
    "    # Convert confusion matrix to DataFrame\n",
    "    matrix_df = pd.DataFrame(confusion_matrix(y_test_resampled, y_pred.round()))\n",
    "    \n",
    "    # Extract metrics for class 1\n",
    "    metrics_1 = report_df.loc['1', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract metrics for class 2\n",
    "    metrics_2 = report_df.loc['2', ['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Extract TP, TN, FP, FN counts from the confusion matrix DataFrame\n",
    "    TP = matrix_df.loc[0, 0]\n",
    "    TN = matrix_df.loc[1, 1]\n",
    "    FP = matrix_df.loc[1, 0]\n",
    "    FN = matrix_df.loc[0, 1]\n",
    "    \n",
    "    new_metric_row = {\n",
    "    'dataset': 'DF8',\n",
    "    'model' : 'Word2Vec - Linear Regression',\n",
    "    'data balancing technique' : 'Random Over Sampler',\n",
    "    'fold' : i,\n",
    "    'precision_1': metrics_1['precision'],\n",
    "    'precision_2': metrics_2['precision'],\n",
    "    'recall_1': metrics_1['recall'],\n",
    "    'recall_2': metrics_2['recall'],\n",
    "    'f1-score_1': metrics_1['f1-score'],\n",
    "    'f1-score_2': metrics_2['f1-score'],\n",
    "    'support_1': metrics_1['support'],\n",
    "    'support_2': metrics_2['support'],\n",
    "    'TP' : TP,\n",
    "    'FP' : FP,\n",
    "    'TN' : TN,\n",
    "    'FN' : FN\n",
    "    }\n",
    "    \n",
    "    i = i + 1\n",
    "    \n",
    "    #combined_metrics = combined_metrics.append(new_metric_row, ignore_index=True)\n",
    "    combined_metrics.loc[len(combined_metrics)] = new_metric_row\n",
    "    \n",
    "    \n",
    "    # Append evaluation metrics to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "# Calculate mean evaluation metrics across all folds\n",
    "mean_accuracy = np.mean(accuracy_scores) #sum(accuracy_scores) / len(accuracy_scores)\n",
    "mean_precision = np.mean(precision_scores) #sum(precision_scores) / len(precision_scores)\n",
    "mean_recall = np.mean(recall_scores) #sum(recall_scores) / len(recall_scores)\n",
    "mean_f1 = np.mean(f1_scores) #sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print('Mean Accuracy: {:.2f}'.format(mean_accuracy))\n",
    "print('Mean Precision: {:.2f}'.format(mean_precision))\n",
    "print('Mean Recall: {:.2f}'.format(mean_recall))\n",
    "print('Mean F1-Score: {:.2f}'.format(mean_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca7e58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.918167</td>\n",
       "      <td>0.951626</td>\n",
       "      <td>0.948436</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>600</td>\n",
       "      <td>50</td>\n",
       "      <td>561</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.995177</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>0.968701</td>\n",
       "      <td>0.966942</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>619</td>\n",
       "      <td>37</td>\n",
       "      <td>585</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.966565</td>\n",
       "      <td>0.964630</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>636</td>\n",
       "      <td>40</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.935199</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.986025</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>0.959940</td>\n",
       "      <td>0.957702</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>635</td>\n",
       "      <td>44</td>\n",
       "      <td>600</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.988487</td>\n",
       "      <td>0.989080</td>\n",
       "      <td>0.937598</td>\n",
       "      <td>0.964259</td>\n",
       "      <td>0.962370</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>634</td>\n",
       "      <td>40</td>\n",
       "      <td>601</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.937593</td>\n",
       "      <td>0.985173</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.934375</td>\n",
       "      <td>0.961158</td>\n",
       "      <td>0.959102</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>631</td>\n",
       "      <td>42</td>\n",
       "      <td>598</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.939568</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>0.989394</td>\n",
       "      <td>0.936364</td>\n",
       "      <td>0.963838</td>\n",
       "      <td>0.961868</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>653</td>\n",
       "      <td>42</td>\n",
       "      <td>618</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.988818</td>\n",
       "      <td>0.989552</td>\n",
       "      <td>0.923881</td>\n",
       "      <td>0.958092</td>\n",
       "      <td>0.955247</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>663</td>\n",
       "      <td>51</td>\n",
       "      <td>619</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.928876</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.986405</td>\n",
       "      <td>0.924471</td>\n",
       "      <td>0.956777</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>653</td>\n",
       "      <td>50</td>\n",
       "      <td>612</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.991639</td>\n",
       "      <td>0.992212</td>\n",
       "      <td>0.923676</td>\n",
       "      <td>0.959337</td>\n",
       "      <td>0.956452</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>637</td>\n",
       "      <td>49</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "20     DF8  Word2Vec - Linear Regression      Random Over Sampler     1   \n",
       "21     DF8  Word2Vec - Linear Regression      Random Over Sampler     2   \n",
       "22     DF8  Word2Vec - Linear Regression      Random Over Sampler     3   \n",
       "23     DF8  Word2Vec - Linear Regression      Random Over Sampler     4   \n",
       "24     DF8  Word2Vec - Linear Regression      Random Over Sampler     5   \n",
       "25     DF8  Word2Vec - Linear Regression      Random Over Sampler     6   \n",
       "26     DF8  Word2Vec - Linear Regression      Random Over Sampler     7   \n",
       "27     DF8  Word2Vec - Linear Regression      Random Over Sampler     8   \n",
       "28     DF8  Word2Vec - Linear Regression      Random Over Sampler     9   \n",
       "29     DF8  Word2Vec - Linear Regression      Random Over Sampler    10   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "20     0.923077     0.980769  0.981997  0.918167    0.951626    0.948436   \n",
       "21     0.943598     0.994898  0.995177  0.940514    0.968701    0.966942   \n",
       "22     0.940828     0.993377  0.993750  0.937500    0.966565    0.964630   \n",
       "23     0.935199     0.985222  0.986025  0.931677    0.959940    0.957702   \n",
       "24     0.940653     0.988487  0.989080  0.937598    0.964259    0.962370   \n",
       "25     0.937593     0.985173  0.985938  0.934375    0.961158    0.959102   \n",
       "26     0.939568     0.988800  0.989394  0.936364    0.963838    0.961868   \n",
       "27     0.928571     0.988818  0.989552  0.923881    0.958092    0.955247   \n",
       "28     0.928876     0.985507  0.986405  0.924471    0.956777    0.954014   \n",
       "29     0.928571     0.991639  0.992212  0.923676    0.959337    0.956452   \n",
       "\n",
       "    support_1  support_2   TP  FP   TN  FN  \n",
       "20      611.0      611.0  600  50  561  11  \n",
       "21      622.0      622.0  619  37  585   3  \n",
       "22      640.0      640.0  636  40  600   4  \n",
       "23      644.0      644.0  635  44  600   9  \n",
       "24      641.0      641.0  634  40  601   7  \n",
       "25      640.0      640.0  631  42  598   9  \n",
       "26      660.0      660.0  653  42  618   7  \n",
       "27      670.0      670.0  663  51  619   7  \n",
       "28      662.0      662.0  653  50  612   9  \n",
       "29      642.0      642.0  637  49  593   5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics[(combined_metrics['data balancing technique'] == 'Random Over Sampler') & (combined_metrics['dataset'] == 'DF8')]#.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd948ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values 'DF7' with 'DF8' in the 'dataset' column\n",
    "combined_metrics['dataset'] = combined_metrics['dataset'].replace('DF7', 'DF8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b04f0",
   "metadata": {},
   "source": [
    "## Performance metrics for the Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1158fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>data balancing technique</th>\n",
       "      <th>fold</th>\n",
       "      <th>precision_1</th>\n",
       "      <th>precision_2</th>\n",
       "      <th>recall_1</th>\n",
       "      <th>recall_2</th>\n",
       "      <th>f1-score_1</th>\n",
       "      <th>f1-score_2</th>\n",
       "      <th>support_1</th>\n",
       "      <th>support_2</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971985</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.828151</td>\n",
       "      <td>0.985794</td>\n",
       "      <td>0.905998</td>\n",
       "      <td>3643.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>3643</td>\n",
       "      <td>105</td>\n",
       "      <td>506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>0.976869</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.988299</td>\n",
       "      <td>0.925734</td>\n",
       "      <td>3632.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>3632</td>\n",
       "      <td>86</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0.972551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.840625</td>\n",
       "      <td>0.986085</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>3614.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3614</td>\n",
       "      <td>102</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>0.966274</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.982848</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>3610.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>3610</td>\n",
       "      <td>126</td>\n",
       "      <td>518</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>0.977279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.868955</td>\n",
       "      <td>0.988509</td>\n",
       "      <td>0.929883</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>3613</td>\n",
       "      <td>84</td>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>0.976486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.864062</td>\n",
       "      <td>0.988103</td>\n",
       "      <td>0.927075</td>\n",
       "      <td>3613.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>3613</td>\n",
       "      <td>87</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>0.972395</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.845455</td>\n",
       "      <td>0.986004</td>\n",
       "      <td>0.916256</td>\n",
       "      <td>3593.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>3593</td>\n",
       "      <td>102</td>\n",
       "      <td>558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>0.969951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.834328</td>\n",
       "      <td>0.984746</td>\n",
       "      <td>0.909683</td>\n",
       "      <td>3583.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>3583</td>\n",
       "      <td>111</td>\n",
       "      <td>559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>0.970270</td>\n",
       "      <td>0.998192</td>\n",
       "      <td>0.999722</td>\n",
       "      <td>0.833837</td>\n",
       "      <td>0.984776</td>\n",
       "      <td>0.908642</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>3590</td>\n",
       "      <td>110</td>\n",
       "      <td>552</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819315</td>\n",
       "      <td>0.984192</td>\n",
       "      <td>0.900685</td>\n",
       "      <td>3611.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>3611</td>\n",
       "      <td>116</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.934985</td>\n",
       "      <td>0.987847</td>\n",
       "      <td>0.988543</td>\n",
       "      <td>0.931260</td>\n",
       "      <td>0.961018</td>\n",
       "      <td>0.958719</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>604</td>\n",
       "      <td>42</td>\n",
       "      <td>569</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.949464</td>\n",
       "      <td>0.996616</td>\n",
       "      <td>0.996785</td>\n",
       "      <td>0.946945</td>\n",
       "      <td>0.972549</td>\n",
       "      <td>0.971146</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>620</td>\n",
       "      <td>33</td>\n",
       "      <td>589</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.942051</td>\n",
       "      <td>0.990115</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>0.939063</td>\n",
       "      <td>0.965727</td>\n",
       "      <td>0.963913</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>634</td>\n",
       "      <td>39</td>\n",
       "      <td>601</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.945104</td>\n",
       "      <td>0.988599</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.942547</td>\n",
       "      <td>0.966616</td>\n",
       "      <td>0.965024</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>637</td>\n",
       "      <td>37</td>\n",
       "      <td>607</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.948949</td>\n",
       "      <td>0.985390</td>\n",
       "      <td>0.985959</td>\n",
       "      <td>0.946958</td>\n",
       "      <td>0.967100</td>\n",
       "      <td>0.965792</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>632</td>\n",
       "      <td>34</td>\n",
       "      <td>607</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.943368</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.989062</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.965675</td>\n",
       "      <td>0.963971</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>633</td>\n",
       "      <td>38</td>\n",
       "      <td>602</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.942363</td>\n",
       "      <td>0.990415</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.966027</td>\n",
       "      <td>0.964230</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>654</td>\n",
       "      <td>40</td>\n",
       "      <td>620</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.931180</td>\n",
       "      <td>0.988854</td>\n",
       "      <td>0.989552</td>\n",
       "      <td>0.926866</td>\n",
       "      <td>0.959479</td>\n",
       "      <td>0.956857</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>663</td>\n",
       "      <td>49</td>\n",
       "      <td>621</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.930398</td>\n",
       "      <td>0.988710</td>\n",
       "      <td>0.989426</td>\n",
       "      <td>0.925982</td>\n",
       "      <td>0.959004</td>\n",
       "      <td>0.956318</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>655</td>\n",
       "      <td>49</td>\n",
       "      <td>613</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Under Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.941704</td>\n",
       "      <td>0.980488</td>\n",
       "      <td>0.981308</td>\n",
       "      <td>0.939252</td>\n",
       "      <td>0.961098</td>\n",
       "      <td>0.959427</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>630</td>\n",
       "      <td>39</td>\n",
       "      <td>603</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.918167</td>\n",
       "      <td>0.951626</td>\n",
       "      <td>0.948436</td>\n",
       "      <td>611.0</td>\n",
       "      <td>611.0</td>\n",
       "      <td>600</td>\n",
       "      <td>50</td>\n",
       "      <td>561</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>2</td>\n",
       "      <td>0.943598</td>\n",
       "      <td>0.994898</td>\n",
       "      <td>0.995177</td>\n",
       "      <td>0.940514</td>\n",
       "      <td>0.968701</td>\n",
       "      <td>0.966942</td>\n",
       "      <td>622.0</td>\n",
       "      <td>622.0</td>\n",
       "      <td>619</td>\n",
       "      <td>37</td>\n",
       "      <td>585</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>3</td>\n",
       "      <td>0.940828</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>0.993750</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.966565</td>\n",
       "      <td>0.964630</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>636</td>\n",
       "      <td>40</td>\n",
       "      <td>600</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>4</td>\n",
       "      <td>0.935199</td>\n",
       "      <td>0.985222</td>\n",
       "      <td>0.986025</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>0.959940</td>\n",
       "      <td>0.957702</td>\n",
       "      <td>644.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>635</td>\n",
       "      <td>44</td>\n",
       "      <td>600</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>5</td>\n",
       "      <td>0.940653</td>\n",
       "      <td>0.988487</td>\n",
       "      <td>0.989080</td>\n",
       "      <td>0.937598</td>\n",
       "      <td>0.964259</td>\n",
       "      <td>0.962370</td>\n",
       "      <td>641.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>634</td>\n",
       "      <td>40</td>\n",
       "      <td>601</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>6</td>\n",
       "      <td>0.937593</td>\n",
       "      <td>0.985173</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.934375</td>\n",
       "      <td>0.961158</td>\n",
       "      <td>0.959102</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>631</td>\n",
       "      <td>42</td>\n",
       "      <td>598</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>7</td>\n",
       "      <td>0.939568</td>\n",
       "      <td>0.988800</td>\n",
       "      <td>0.989394</td>\n",
       "      <td>0.936364</td>\n",
       "      <td>0.963838</td>\n",
       "      <td>0.961868</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>653</td>\n",
       "      <td>42</td>\n",
       "      <td>618</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>8</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.988818</td>\n",
       "      <td>0.989552</td>\n",
       "      <td>0.923881</td>\n",
       "      <td>0.958092</td>\n",
       "      <td>0.955247</td>\n",
       "      <td>670.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>663</td>\n",
       "      <td>51</td>\n",
       "      <td>619</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>9</td>\n",
       "      <td>0.928876</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.986405</td>\n",
       "      <td>0.924471</td>\n",
       "      <td>0.956777</td>\n",
       "      <td>0.954014</td>\n",
       "      <td>662.0</td>\n",
       "      <td>662.0</td>\n",
       "      <td>653</td>\n",
       "      <td>50</td>\n",
       "      <td>612</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>DF8</td>\n",
       "      <td>Word2Vec - Linear Regression</td>\n",
       "      <td>Random Over Sampler</td>\n",
       "      <td>10</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.991639</td>\n",
       "      <td>0.992212</td>\n",
       "      <td>0.923676</td>\n",
       "      <td>0.959337</td>\n",
       "      <td>0.956452</td>\n",
       "      <td>642.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>637</td>\n",
       "      <td>49</td>\n",
       "      <td>593</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset                         model data balancing technique  fold  \\\n",
       "0      DF8  Word2Vec - Linear Regression                     None     1   \n",
       "1      DF8  Word2Vec - Linear Regression                     None     2   \n",
       "2      DF8  Word2Vec - Linear Regression                     None     3   \n",
       "3      DF8  Word2Vec - Linear Regression                     None     4   \n",
       "4      DF8  Word2Vec - Linear Regression                     None     5   \n",
       "5      DF8  Word2Vec - Linear Regression                     None     6   \n",
       "6      DF8  Word2Vec - Linear Regression                     None     7   \n",
       "7      DF8  Word2Vec - Linear Regression                     None     8   \n",
       "8      DF8  Word2Vec - Linear Regression                     None     9   \n",
       "9      DF8  Word2Vec - Linear Regression                     None    10   \n",
       "10     DF8  Word2Vec - Linear Regression     Random Under Sampler     1   \n",
       "11     DF8  Word2Vec - Linear Regression     Random Under Sampler     2   \n",
       "12     DF8  Word2Vec - Linear Regression     Random Under Sampler     3   \n",
       "13     DF8  Word2Vec - Linear Regression     Random Under Sampler     4   \n",
       "14     DF8  Word2Vec - Linear Regression     Random Under Sampler     5   \n",
       "15     DF8  Word2Vec - Linear Regression     Random Under Sampler     6   \n",
       "16     DF8  Word2Vec - Linear Regression     Random Under Sampler     7   \n",
       "17     DF8  Word2Vec - Linear Regression     Random Under Sampler     8   \n",
       "18     DF8  Word2Vec - Linear Regression     Random Under Sampler     9   \n",
       "19     DF8  Word2Vec - Linear Regression     Random Under Sampler    10   \n",
       "20     DF8  Word2Vec - Linear Regression      Random Over Sampler     1   \n",
       "21     DF8  Word2Vec - Linear Regression      Random Over Sampler     2   \n",
       "22     DF8  Word2Vec - Linear Regression      Random Over Sampler     3   \n",
       "23     DF8  Word2Vec - Linear Regression      Random Over Sampler     4   \n",
       "24     DF8  Word2Vec - Linear Regression      Random Over Sampler     5   \n",
       "25     DF8  Word2Vec - Linear Regression      Random Over Sampler     6   \n",
       "26     DF8  Word2Vec - Linear Regression      Random Over Sampler     7   \n",
       "27     DF8  Word2Vec - Linear Regression      Random Over Sampler     8   \n",
       "28     DF8  Word2Vec - Linear Regression      Random Over Sampler     9   \n",
       "29     DF8  Word2Vec - Linear Regression      Random Over Sampler    10   \n",
       "\n",
       "    precision_1  precision_2  recall_1  recall_2  f1-score_1  f1-score_2  \\\n",
       "0      0.971985     1.000000  1.000000  0.828151    0.985794    0.905998   \n",
       "1      0.976869     1.000000  1.000000  0.861736    0.988299    0.925734   \n",
       "2      0.972551     1.000000  1.000000  0.840625    0.986085    0.913413   \n",
       "3      0.966274     1.000000  1.000000  0.804348    0.982848    0.891566   \n",
       "4      0.977279     1.000000  1.000000  0.868955    0.988509    0.929883   \n",
       "5      0.976486     1.000000  1.000000  0.864062    0.988103    0.927075   \n",
       "6      0.972395     1.000000  1.000000  0.845455    0.986004    0.916256   \n",
       "7      0.969951     1.000000  1.000000  0.834328    0.984746    0.909683   \n",
       "8      0.970270     0.998192  0.999722  0.833837    0.984776    0.908642   \n",
       "9      0.968876     1.000000  1.000000  0.819315    0.984192    0.900685   \n",
       "10     0.934985     0.987847  0.988543  0.931260    0.961018    0.958719   \n",
       "11     0.949464     0.996616  0.996785  0.946945    0.972549    0.971146   \n",
       "12     0.942051     0.990115  0.990625  0.939063    0.965727    0.963913   \n",
       "13     0.945104     0.988599  0.989130  0.942547    0.966616    0.965024   \n",
       "14     0.948949     0.985390  0.985959  0.946958    0.967100    0.965792   \n",
       "15     0.943368     0.988506  0.989062  0.940625    0.965675    0.963971   \n",
       "16     0.942363     0.990415  0.990909  0.939394    0.966027    0.964230   \n",
       "17     0.931180     0.988854  0.989552  0.926866    0.959479    0.956857   \n",
       "18     0.930398     0.988710  0.989426  0.925982    0.959004    0.956318   \n",
       "19     0.941704     0.980488  0.981308  0.939252    0.961098    0.959427   \n",
       "20     0.923077     0.980769  0.981997  0.918167    0.951626    0.948436   \n",
       "21     0.943598     0.994898  0.995177  0.940514    0.968701    0.966942   \n",
       "22     0.940828     0.993377  0.993750  0.937500    0.966565    0.964630   \n",
       "23     0.935199     0.985222  0.986025  0.931677    0.959940    0.957702   \n",
       "24     0.940653     0.988487  0.989080  0.937598    0.964259    0.962370   \n",
       "25     0.937593     0.985173  0.985938  0.934375    0.961158    0.959102   \n",
       "26     0.939568     0.988800  0.989394  0.936364    0.963838    0.961868   \n",
       "27     0.928571     0.988818  0.989552  0.923881    0.958092    0.955247   \n",
       "28     0.928876     0.985507  0.986405  0.924471    0.956777    0.954014   \n",
       "29     0.928571     0.991639  0.992212  0.923676    0.959337    0.956452   \n",
       "\n",
       "    support_1  support_2    TP   FP   TN  FN  \n",
       "0      3643.0      611.0  3643  105  506   0  \n",
       "1      3632.0      622.0  3632   86  536   0  \n",
       "2      3614.0      640.0  3614  102  538   0  \n",
       "3      3610.0      644.0  3610  126  518   0  \n",
       "4      3613.0      641.0  3613   84  557   0  \n",
       "5      3613.0      640.0  3613   87  553   0  \n",
       "6      3593.0      660.0  3593  102  558   0  \n",
       "7      3583.0      670.0  3583  111  559   0  \n",
       "8      3591.0      662.0  3590  110  552   1  \n",
       "9      3611.0      642.0  3611  116  526   0  \n",
       "10      611.0      611.0   604   42  569   7  \n",
       "11      622.0      622.0   620   33  589   2  \n",
       "12      640.0      640.0   634   39  601   6  \n",
       "13      644.0      644.0   637   37  607   7  \n",
       "14      641.0      641.0   632   34  607   9  \n",
       "15      640.0      640.0   633   38  602   7  \n",
       "16      660.0      660.0   654   40  620   6  \n",
       "17      670.0      670.0   663   49  621   7  \n",
       "18      662.0      662.0   655   49  613   7  \n",
       "19      642.0      642.0   630   39  603  12  \n",
       "20      611.0      611.0   600   50  561  11  \n",
       "21      622.0      622.0   619   37  585   3  \n",
       "22      640.0      640.0   636   40  600   4  \n",
       "23      644.0      644.0   635   44  600   9  \n",
       "24      641.0      641.0   634   40  601   7  \n",
       "25      640.0      640.0   631   42  598   9  \n",
       "26      660.0      660.0   653   42  618   7  \n",
       "27      670.0      670.0   663   51  619   7  \n",
       "28      662.0      662.0   653   50  612   9  \n",
       "29      642.0      642.0   637   49  593   5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8905e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "combined_metrics.to_csv('Output Data/Word2Vec - Linear Regression2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca50962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
